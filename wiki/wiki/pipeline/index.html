<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>System Architecture &amp; Pipeline | FileSense</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="System Architecture &amp; Pipeline" />
<meta property="og:locale" content="en_US" />
<meta property="og:site_name" content="FileSense" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="System Architecture &amp; Pipeline" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"System Architecture &amp; Pipeline","url":"/wiki/pipeline/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=ff113580487d33068b3216f649af5514f0da9c75">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="/">FileSense</a></h1>
      

      <h1 id="Ô∏è-system-architecture">üèóÔ∏è System Architecture</h1>

<p>Understanding how FileSense processes and classifies files.</p>

<hr />

<h2 id="-high-level-overview">üìä High-Level Overview</h2>

<pre><code class="language-mermaid">flowchart TB
    subgraph Input
        A[üìÑ Input Files&lt;br/&gt;PDF, DOCX, TXT, MD]
    end
    
    subgraph Extraction
        B[üìù Text Extraction&lt;br/&gt;pdfplumber, python-docx]
        C[üîç Fallback Extraction&lt;br/&gt;Middle pages, OCR]
    end
    
    subgraph RLAgent [üß† RL Agent]
        D[ü§ñ Epsilon-Greedy Policy&lt;br/&gt;Exploit vs Explore]
    end
    
    subgraph Embedding
        E[üî¢ SBERT Encoding&lt;br/&gt;BGE-Base v1.5&lt;br/&gt;768 dimensions]
    end
    
    subgraph Search
        F[üéØ FAISS Vector Search&lt;br/&gt;IndexFlatIP&lt;br/&gt;Cosine Similarity]
        G{Similarity ‚â• 0.40?}
    end
    
    subgraph Classification
        H[‚úÖ High Confidence&lt;br/&gt;Assign to Folder]
        I[‚ú® Generative Fallback&lt;br/&gt;Ask Gemini]
    end
    
    subgraph Update
        J[üíæ Update Labels&lt;br/&gt;folder_labels.json]
        K[üîÑ Rebuild Index&lt;br/&gt;FAISS re-indexing]
        L[üîÅ Re-classify]
    end
    
    subgraph Output
        M[üìÅ Move to Sorted Folder]
    end
    
    A --&gt; B
    B --&gt; C
    C --&gt; D
    D --&gt;|Get State| E
    E --&gt; F
    F --&gt; G
    G --&gt;|Yes| H
    G --&gt;|No| I
    I --&gt; J
    J --&gt; K
    K --&gt; L
    L --&gt; F
    H --&gt; M
</code></pre>

<hr />

<h2 id="-processing-pipeline">üîÑ Processing Pipeline</h2>

<h3 id="step-1-text-extraction">Step 1: Text Extraction</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/extract_text.py</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">extract_text</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">"""
    Extract text from PDF, DOCX, or TXT files.
    
    Args:
        file_path: Path to the file
        fallback: If True, extract from middle pages (avoid TOC)
    
    Returns:
        Extracted text (max 2000 chars)
    """</span>
</code></pre></div></div>

<p><strong>Features:</strong></p>
<ul>
  <li><strong>PDF:</strong> Uses <code class="language-plaintext highlighter-rouge">pdfplumber</code> with smart page selection</li>
  <li><strong>DOCX:</strong> Extracts paragraphs with <code class="language-plaintext highlighter-rouge">python-docx</code></li>
  <li><strong>TXT:</strong> Direct file reading with encoding detection</li>
  <li><strong>Fallback:</strong> Starts from middle pages to avoid table of contents</li>
  <li><strong>Quality scoring:</strong> Filters low-quality pages</li>
  <li><strong>Header/footer removal:</strong> Crops margins to get core content</li>
</ul>

<p><strong>Configuration:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PDF_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'MAX_INPUT_CHARS'</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span>        <span class="c1"># Max text length
</span>    <span class="s">'QUALITY_THRESHOLD'</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>       <span class="c1"># Min page quality
</span>    <span class="s">'START_PAGE_ASSUMPTION'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>     <span class="c1"># Skip first N pages
</span>    <span class="s">'HEADER_FOOTER_MARGIN'</span><span class="p">:</span> <span class="mi">70</span><span class="p">,</span>     <span class="c1"># Crop margins (pixels)
</span>    <span class="s">'MIN_LINE_LENGTH'</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span>          <span class="c1"># Min line length
</span><span class="p">}</span>
</code></pre></div></div>

<h3 id="step-2-the-rl-agent-decision-maker">Step 2: The RL Agent (Decision Maker)</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/RL/rl_policy.py</code></p>

<p>Before any expensive operations, the <strong>Reinforcement Learning Agent</strong> decides the optimal strategy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EpsilonGreedyBandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s">"""
        Decide whether to EXPLOIT (use existing knowledge) or EXPLORE (try new generation).
        
        Policy A: High Accuracy (Liberal use of API)
        Policy B: Balanced
        Policy C: Efficient (Vector Search only)
        """</span>
</code></pre></div></div>

<p><strong>Why RL?</strong></p>
<ul>
  <li><strong>Cost Optimization:</strong> Prevents unnecessary API calls for simple files.</li>
  <li><strong>Latency Reduction:</strong> Skips generation if vector confidence is high.</li>
  <li><strong>Adaptability:</strong> Learns from user feedback (folder moves/renames).</li>
</ul>

<h3 id="step-3-embedding-generation">Step 3: Embedding Generation</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/classify_process_file.py</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">"BAAI/bge-base-en-v1.5"</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cpu"</span><span class="p">)</span>
<span class="n">text_emb</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="n">text</span><span class="p">],</span> <span class="n">normalize_embeddings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Model Details:</strong>
<strong>Model Details:</strong></p>
<ul>
  <li><strong>Name:</strong> BAAI/bge-base-en-v1.5</li>
  <li><strong>Dimensions:</strong> 768</li>
  <li><strong>Normalization:</strong> L2 normalized for cosine similarity</li>
  <li><strong>Performance:</strong> ~0.1s per encoding (Batch optimized)</li>
  <li><strong>Size:</strong> 440MB (cached after first download)</li>
</ul>

<blockquote>
  <p><strong>Why this model?</strong></p>

  <p>Benchmark testing confirmed <code class="language-plaintext highlighter-rouge">bge-base-en-v1.5</code> is <strong>2x faster</strong> than MPNet and significantly more accurate than MiniLM. See <a href="/FileSense/wiki/metrics/#25-reference-model-comparison">Model Comparison</a>.</p>
</blockquote>

<h3 id="step-4-vector-similarity-search">Step 4: Vector Similarity Search</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/classify_process_file.py</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Search FAISS index
</span><span class="n">D</span><span class="p">,</span> <span class="n">I</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="n">text_emb</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Top 10 matches
</span>
<span class="c1"># Calculate similarities
</span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">sim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">all_sims</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sim</span>
</code></pre></div></div>

<p><strong>FAISS Configuration:</strong></p>
<ul>
  <li><strong>Index Type:</strong> IndexFlatIP (Inner Product)</li>
  <li><strong>Metric:</strong> Cosine similarity (via normalized embeddings)</li>
  <li><strong>Top-K:</strong> Retrieves top 10 matches</li>
  <li><strong>Boosting:</strong> Filename and keyword matching add bonus scores</li>
</ul>

<blockquote>
  <p><strong>Why Cosine Similarity?</strong></p>

  <p>We use cosine similarity because it compares <strong>semantic direction</strong> instead of vector magnitude. This avoids bias from unequal text lengths and keyword-heavy folder descriptions. It aligns better with how sentence-embedding models are trained (on angular distance), ensuring more accurate matching of file content to topic labels.</p>
</blockquote>

<p><strong>Keyword Boosting:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">FILENAME_BOOST</span> <span class="o">=</span> <span class="mf">0.2</span>  <span class="c1"># If label appears in filename
</span><span class="n">TEXT_BOOST</span> <span class="o">=</span> <span class="mf">0.1</span>      <span class="c1"># If keyword appears in filename
</span></code></pre></div></div>

<h3 id="step-5-classification-decision">Step 5: Classification Decision</h3>

<p><strong>Thresholds:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">THRESHOLD</span> <span class="o">=</span> <span class="mf">0.4</span>                    <span class="c1"># Main threshold
</span><span class="n">low_confidence_threshold</span> <span class="o">=</span> <span class="mf">0.35</span>    <span class="c1"># Fallback threshold
</span></code></pre></div></div>

<p><strong>Decision Logic:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">similarity</span> <span class="o">&gt;=</span> <span class="mf">0.40</span><span class="p">:</span>
    <span class="c1"># High confidence - classify immediately
</span>    <span class="k">return</span> <span class="n">label</span><span class="p">,</span> <span class="n">similarity</span>

<span class="k">elif</span> <span class="n">similarity</span> <span class="o">&gt;=</span> <span class="mf">0.35</span><span class="p">:</span>
    <span class="c1"># Medium confidence - accept with warning
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"[!] Low confidence but accepting as fallback"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">label</span><span class="p">,</span> <span class="n">similarity</span>

<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Low confidence - generate new label
</span>    <span class="k">if</span> <span class="n">allow_generation</span><span class="p">:</span>
        <span class="n">generate_new_label_via_gemini</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s">"Uncategorized"</span><span class="p">,</span> <span class="mf">0.0</span>
</code></pre></div></div>

<h3 id="step-6-label-generation-gemini">Step 6: Label Generation (Gemini)</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/generate_label.py</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_folder_label</span><span class="p">(</span><span class="n">target_text</span><span class="p">,</span> <span class="n">forced_label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""
    Generate or update folder label using Gemini.
    
    Args:
        target_text: Document text + filename
        forced_label: Optional manual label override
    
    Returns:
        {
            "folder_label": "Physics",
            "description": "mechanics, forces, energy, ...",
            "keywords": "physics, motion, force, ..."
        }
    """</span>
</code></pre></div></div>

<p><strong>Prompt Strategy:</strong></p>
<ul>
  <li><strong>Keyword-based descriptions</strong> (proven superior to natural language)</li>
  <li><strong>Existing labels context</strong> (prefer reuse over new creation)</li>
  <li><strong>Broad categorization</strong> (Physics, not Thermodynamics)</li>
  <li><strong>15 focused examples</strong> covering edge cases</li>
</ul>

<p><strong>Merging Logic:</strong>
When a label already exists, FileSense <strong>merges</strong> new terminology with existing:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">merge_folder_metadata</span><span class="p">(</span><span class="n">folder_label</span><span class="p">,</span> <span class="n">old_desc</span><span class="p">,</span> <span class="n">old_kw</span><span class="p">,</span> <span class="n">new_desc</span><span class="p">,</span> <span class="n">new_kw</span><span class="p">):</span>
    <span class="s">"""
    Intelligently merge metadata preserving all unique terms.
    
    Hard requirement: NO terms from old metadata are lost.
    """</span>
</code></pre></div></div>

<h3 id="step-7-index-rebuild">Step 7: Index Rebuild</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/create_index.py</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_faiss_index</span><span class="p">():</span>
    <span class="c1"># Load labels
</span>    <span class="n">folder_data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"folder_labels.json"</span><span class="p">)</span>
    
    <span class="c1"># Generate embeddings
</span>    <span class="n">combined_desc</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">desc</span><span class="si">}</span><span class="s">"</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">desc</span> <span class="ow">in</span> <span class="n">folder_data</span><span class="p">.</span><span class="n">items</span><span class="p">()]</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">combined_desc</span><span class="p">,</span> <span class="n">normalize_embeddings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Build FAISS index
</span>    <span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="p">.</span><span class="n">IndexFlatIP</span><span class="p">(</span><span class="mi">768</span><span class="p">)</span>  <span class="c1"># 768 dimensions
</span>    <span class="n">index</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    
    <span class="c1"># Save to disk
</span>    <span class="n">faiss</span><span class="p">.</span><span class="n">write_index</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="s">"folder_embeddings.faiss"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Performance:</strong></p>
<ul>
  <li><strong>Speed:</strong> ~1.5s for 10 labels</li>
  <li><strong>Scaling:</strong> Linear with number of labels</li>
  <li><strong>Memory:</strong> ~3KB per label</li>
</ul>

<hr />

<h2 id="Ô∏è-data-structures">üóÇÔ∏è Data Structures</h2>

<h3 id="folder_labelsjson">folder_labels.json</h3>

<p><strong>Format:</strong></p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"Physics"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mechanics, thermodynamics, optics, electromagnetism, quantum mechanics, relativity, forces, energy, motion, waves, heat, light, electricity, magnetism, gravity, laboratory experiments, scientific formulas, physical laws Keywords: physics, mechanics, thermodynamics, optics, quantum, energy, force, motion"</span><span class="p">,</span><span class="w">
  
  </span><span class="nl">"Chemistry"</span><span class="p">:</span><span class="w"> </span><span class="s2">"organic chemistry, inorganic chemistry, chemical reactions, molecular structure, stoichiometry, titration, synthesis, bonding, compounds, purification, acids, bases Keywords: chemistry, organic, reaction, chemical, lab, synthesis, molecule"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><strong>Structure:</strong></p>
<ul>
  <li><strong>Key:</strong> Folder label (broad category)</li>
  <li><strong>Value:</strong> <code class="language-plaintext highlighter-rouge">{description} Keywords: {keywords}</code></li>
  <li><strong>Description:</strong> 20-40 comma-separated terms</li>
  <li><strong>Keywords:</strong> 8-12 high-value search terms</li>
</ul>

<blockquote>
  <p><strong>Why keywords, not natural language?</strong></p>

  <p>Extensive testing showed keyword-based descriptions outperform natural language by <strong>+32% accuracy</strong>. See <a href="/FileSense/wiki/NL_VS_OG/">NL vs Keywords Study</a>.</p>
</blockquote>

<h3 id="folder_embeddingsfaiss">folder_embeddings.faiss</h3>

<p><strong>Binary format</strong> containing:</p>
<ul>
  <li>Vector embeddings for each label</li>
  <li>Index structure for fast search</li>
  <li>Metadata for reconstruction</li>
</ul>

<p><strong>Size:</strong> ~3KB per label (768 float32 values)</p>

<hr />

<h2 id="-multithreading-architecture">üßµ Multithreading Architecture</h2>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/multhread.py</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_multiple</span><span class="p">(</span><span class="n">files_dir</span><span class="p">,</span> <span class="n">max_threads</span><span class="p">,</span> <span class="n">testing</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">allow_generation</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s">"""
    Process files in parallel with thread pool.
    
    Args:
        files_dir: Directory containing files
        max_threads: Maximum concurrent threads
        testing: If True, don't move files
        allow_generation: Allow Gemini label generation
    """</span>
</code></pre></div></div>

<p><strong>Thread Safety:</strong></p>
<ul>
  <li><strong>RLock:</strong> Prevents race conditions during label generation</li>
  <li><strong>FAISS index:</strong> Read-only after loading (thread-safe)</li>
  <li><strong>File I/O:</strong> Each thread processes different files</li>
</ul>

<p><strong>Performance:</strong></p>
<ul>
  <li><strong>Optimal threads:</strong> 4-8 (I/O bound)</li>
  <li><strong>Scaling:</strong> Near-linear up to 8 threads</li>
  <li><strong>Bottleneck:</strong> Text extraction (especially OCR)</li>
</ul>

<hr />

<h2 id="-fallback-mechanisms">üîç Fallback Mechanisms</h2>

<h3 id="1-fallback-text-extraction">1. Fallback Text Extraction</h3>

<p>If initial classification has low confidence:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">similarity</span> <span class="o">&lt;</span> <span class="mf">0.35</span><span class="p">:</span>
    <span class="c1"># Try extracting from middle of document
</span>    <span class="n">fallback_text</span> <span class="o">=</span> <span class="n">extract_text</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">new_sim</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">fallback_text</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">new_sim</span> <span class="o">&gt;</span> <span class="n">similarity</span><span class="p">:</span>
        <span class="c1"># Use fallback result
</span>        <span class="n">text</span> <span class="o">=</span> <span class="n">fallback_text</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="n">new_sim</span>
</code></pre></div></div>

<p><strong>Why?</strong> Many documents have table of contents or cover pages that don‚Äôt represent actual content.</p>

<h3 id="2-threshold-lowering">2. Threshold Lowering</h3>

<p>When generation is disabled:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">allow_generation</span><span class="p">:</span>
    <span class="c1"># Lower threshold by 7%
</span>    <span class="n">current_threshold</span> <span class="o">-=</span> <span class="mf">0.07</span>
    
    <span class="k">if</span> <span class="n">similarity</span> <span class="o">&gt;</span> <span class="n">current_threshold</span><span class="p">:</span>
        <span class="c1"># Accept with lowered threshold
</span>        <span class="k">return</span> <span class="n">label</span><span class="p">,</span> <span class="n">similarity</span>
</code></pre></div></div>

<h3 id="3-manual-input">3. Manual Input</h3>

<p>After max retries:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">retries</span> <span class="o">&gt;=</span> <span class="n">MAX_RETRIES</span><span class="p">:</span>
    <span class="c1"># Ask user for manual label
</span>    <span class="n">manual_label</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="sa">f</span><span class="s">"Enter label for '</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s">': "</span><span class="p">)</span>
    
    <span class="c1"># Optionally generate description
</span>    <span class="k">if</span> <span class="n">user_wants_description</span><span class="p">:</span>
        <span class="n">generate_folder_label</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">forced_label</span><span class="o">=</span><span class="n">manual_label</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="-performance-characteristics">üìä Performance Characteristics</h2>

<h3 id="time-complexity">Time Complexity</h3>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>Complexity</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Text Extraction</td>
      <td>O(n)</td>
      <td>n = file size</td>
    </tr>
    <tr>
      <td>RL Policy Decision</td>
      <td>O(1)</td>
      <td>Table lookup / State check</td>
    </tr>
    <tr>
      <td>Embedding Generation</td>
      <td>O(1)</td>
      <td>Fixed for 2000 chars</td>
    </tr>
    <tr>
      <td>FAISS Search</td>
      <td>O(k)</td>
      <td>k = number of labels</td>
    </tr>
    <tr>
      <td>Label Generation</td>
      <td>O(1)</td>
      <td>API call latency</td>
    </tr>
    <tr>
      <td>Index Rebuild</td>
      <td>O(k)</td>
      <td>k = number of labels</td>
    </tr>
  </tbody>
</table>

<h3 id="space-complexity">Space Complexity</h3>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Size</th>
      <th>Scaling</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Embedding Model</td>
      <td>420MB</td>
      <td>One-time</td>
    </tr>
    <tr>
      <td>FAISS Index</td>
      <td>~3KB/label</td>
      <td>Linear</td>
    </tr>
    <tr>
      <td>folder_labels.json</td>
      <td>~500B/label</td>
      <td>Linear</td>
    </tr>
    <tr>
      <td>Logs</td>
      <td>Variable</td>
      <td>Can be disabled</td>
    </tr>
  </tbody>
</table>

<h3 id="bottlenecks">Bottlenecks</h3>

<ol>
  <li><strong>Text Extraction</strong> (especially OCR)
    <ul>
      <li>Solution: Use more threads</li>
      <li>Solution: Pre-extract text</li>
    </ul>
  </li>
  <li><strong>Gemini API Latency</strong> (~1-2s per call)
    <ul>
      <li>Solution: Batch processing</li>
      <li>Solution: Pre-generate common labels</li>
    </ul>
  </li>
  <li><strong>Model Download</strong> (first run only)
    <ul>
      <li>Solution: Manual download and cache</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="-configuration-points">üîß Configuration Points</h2>

<h3 id="similarity-thresholds">Similarity Thresholds</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/classify_process_file.py</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">THRESHOLD</span> <span class="o">=</span> <span class="mf">0.4</span>                    <span class="c1"># Adjust for strictness
</span><span class="n">low_confidence_threshold</span> <span class="o">=</span> <span class="mf">0.35</span>    <span class="c1"># Fallback threshold
</span></code></pre></div></div>

<h3 id="text-extraction">Text Extraction</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/extract_text.py</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PDF_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'MAX_INPUT_CHARS'</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span>       <span class="c1"># Increase for longer docs
</span>    <span class="s">'QUALITY_THRESHOLD'</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># Lower to accept more pages
</span>    <span class="s">'START_PAGE_ASSUMPTION'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>    <span class="c1"># Skip more/fewer initial pages
</span><span class="p">}</span>
</code></pre></div></div>

<h3 id="embedding-model">Embedding Model</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/create_index.py</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">"BAAI/bge-base-en-v1.5"</span>  <span class="c1"># Change model here
</span></code></pre></div></div>

<h3 id="gemini-settings">Gemini Settings</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/generate_label.py</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MODEL</span> <span class="o">=</span> <span class="s">"gemini-2.5-flash"</span>         <span class="c1"># Gemini model version
</span><span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.5</span>                  <span class="c1"># Creativity (0.0-1.0)
</span></code></pre></div></div>

<h3 id="rl-settings">RL Settings</h3>

<p><strong>File:</strong> <code class="language-plaintext highlighter-rouge">scripts/RL/rl_config.py</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.2</span>          <span class="c1"># Exploration rate
</span><span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.1</span>    <span class="c1"># Q-Learning rate
</span></code></pre></div></div>

<hr />

<h2 id="-design-decisions">üéì Design Decisions</h2>

<h3 id="why-faiss-over-other-vector-dbs">Why FAISS over other vector DBs?</h3>

<ul>
  <li><strong>Fast:</strong> Optimized for similarity search</li>
  <li><strong>Lightweight:</strong> No server required</li>
  <li><strong>Offline:</strong> Works without internet</li>
  <li><strong>Scalable:</strong> Handles thousands of labels efficiently</li>
</ul>

<h3 id="why-sbert-over-other-embeddings">Why SBERT over other embeddings?</h3>

<ul>
  <li><strong>Semantic:</strong> Captures meaning, not just keywords</li>
  <li><strong>Pre-trained:</strong> No training required</li>
  <li><strong>Fast:</strong> ~0.1s per encoding</li>
  <li><strong>Accurate:</strong> Best performance in testing</li>
</ul>

<h3 id="why-keyword-descriptions">Why keyword descriptions?</h3>

<p><strong>Proven through testing:</strong></p>
<ul>
  <li>Keywords: 56% accuracy</li>
  <li>Natural language: 24% accuracy</li>
  <li><strong>+32% improvement</strong> with keywords</li>
</ul>

<p>See <a href="/FileSense/wiki/NL_VS_OG/">NL vs Keywords Study</a> for details.</p>

<h3 id="why-gemini-for-generation">Why Gemini for generation?</h3>

<ul>
  <li><strong>Structured output:</strong> JSON schema support</li>
  <li><strong>Context understanding:</strong> Analyzes document content</li>
  <li><strong>Merging logic:</strong> Preserves existing metadata</li>
  <li><strong>Cost-effective:</strong> Free tier available</li>
</ul>

<hr />

<h2 id="-related-documentation">üìö Related Documentation</h2>

<ul>
  <li><strong><a href="/FileSense/wiki/api-reference/">API Reference</a></strong> - Function documentation</li>
  <li><strong><a href="/FileSense/wiki/metrics/">Performance Metrics</a></strong> - Benchmarks</li>
  <li><strong><a href="/FileSense/wiki/code-structure/">Code Structure</a></strong> - Project organization</li>
</ul>

<hr />

<p><a href="/FileSense/wiki/">‚Üê Back to Home</a></p>


      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  </body>
</html>
