<!DOCTYPE html>
<html lang="">

<title>Reinforcement Learning Architecture | FileSense Documentation</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="color-scheme" content="light dark">
<meta name="author" content="Ayush Bhalerao">
<meta name="generator" content="Jekyll v3.10.0">
<link rel="canonical" href="https://ahhyoushh.github.io/FileSense/wiki/rl/"><link rel="stylesheet" href="/FileSense/assets/css/index.css"><link rel="stylesheet" href="/FileSense/assets/PT-Sans/index.css"><link rel="alternate" href="/FileSense/feed.xml" type="application/atom+xml" title="FileSense Documentation"><link rel="stylesheet" href="/FileSense/assets/css/sidebar.css" media="screen and (min-width: 58em)">
<aside style="display: none">
  <nav>
  
  <a href="/FileSense/wiki/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Home</span>
    </a><a href="/FileSense/wiki/getting-started/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Getting Started</span>
    </a><a href="/FileSense/wiki/faq/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>FAQ</span>
    </a><a href="/FileSense/wiki/pipeline/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Architecture</span>
    </a><a href="/FileSense/wiki/rl/" class="selected">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Reinforcement Learning</span>
    </a><a href="/FileSense/wiki/metrics/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Metrics</span>
    </a><a href="/FileSense/wiki/NL_VS_OG/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>NL vs Keywords</span>
    </a><a href="/FileSense/wiki/lessons-learned/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Lessons Learned</span>
    </a><a href="https://github.com/ahhyoushh/FileSense" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#github"></use></svg>
      <span>GitHub</span>
    </a></nav>
  <footer>Intelligent semantic file organizer powered by SBERT + Gemini</footer>
</aside>


<header class="hover smooth">
  
    <h1 ><a href="/FileSense/">FileSense Documentation</a></h1>
  
  <nav><a href="/FileSense/wiki/">Home</a><a href="/FileSense/wiki/getting-started/">Getting Started</a><a href="/FileSense/wiki/faq/">FAQ</a><a href="/FileSense/wiki/pipeline/">Architecture</a><a href="/FileSense/wiki/rl/">Reinforcement Learning</a><a href="/FileSense/wiki/metrics/">Metrics</a><a href="/FileSense/wiki/NL_VS_OG/">NL vs Keywords</a><a href="/FileSense/wiki/lessons-learned/">Lessons Learned</a><a href="https://github.com/ahhyoushh/FileSense"><svg aria-label="GitHub" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#github"></use></svg></a></nav>
  
</header>

<article>
  <header>
  <h1><a href="/FileSense/wiki/rl/">Reinforcement Learning Architecture</a></h1>
</header>

  <h1 id="reinforcement-learning--the-rate-limit-bottleneck">Reinforcement Learning &amp; The Rate Limit Bottleneck</h1>

<p>## üö® Critical Analysis: Why the API Approach Failed</p>

<p>### 1. The Bottleneck: API Quotas &amp; Latency
 Despite implementing an intelligent RL agent (Epsilon-Greedy Bandit) to minimize API calls (Policy C), the dependency on Google Gemini‚Äôs API proved fatal for the project‚Äôs scalability.</p>

<p><strong>Evidence from Logs (<code class="language-plaintext highlighter-rouge">RL_init.log</code>, <code class="language-plaintext highlighter-rouge">RL_RATE_LIMIT_RAGEBAIT.log</code>):</strong></p>
<ul>
  <li><strong>Severe Rate Limiting (429 RESOURCE_EXHAUSTED):</strong>
    <blockquote>
      <p><code class="language-plaintext highlighter-rouge">Error: 429 RESOURCE_EXHAUSTED ... limit: 20 requests/day ... Please retry in 43.82s</code>
The free/standard tier limits are far too low for a file organizer that might process hundreds of files. A limit of ~20 requests forces the system to sleep more than it works.</p>
    </blockquote>
  </li>
  <li><strong>Service Unavailability (503 UNAVAILABLE):</strong>
    <blockquote>
      <p><code class="language-plaintext highlighter-rouge">Error: 503 UNAVAILABLE ... The model is overloaded.</code>
Even within the quota, the model frequently failed to respond, triggering retry loops that added 10-20 seconds of delay per file.</p>
    </blockquote>
  </li>
  <li><strong>Unacceptable Latency:</strong>
The retry logic and backoff strategies blew up processing times:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">file_005.txt</code>: <strong>57.45s</strong></li>
      <li><code class="language-plaintext highlighter-rouge">file_003.txt</code>: <strong>70.12s</strong></li>
      <li><code class="language-plaintext highlighter-rouge">file_004.txt</code>: <strong>96.77s</strong></li>
      <li><code class="language-plaintext highlighter-rouge">file_007.txt</code>: <strong>123.44s</strong></li>
    </ul>

    <p><em>Compare this to Vector Search (Policy C):</em> <code class="language-plaintext highlighter-rouge">~0.50s</code> per file.</p>
  </li>
</ul>

<p>### 2. Failure of the RL ‚ÄúFix‚Äù
 The RL agent correctly identified <strong>Policy C (No GenAI)</strong> as the optimal policy because it had the highest reward (speed + no errors). However, when the system <em>did</em> need to generate a new label (Exploration or Low Confidence), the API failure broke the entire loop.</p>

<ul>
  <li><strong>The ‚ÄúGap‚Äù:</strong> We cannot rely on the API even for the 10% ‚ÄúExplore‚Äù cases without risking a 60-second freeze.</li>
  <li><strong>Manual Fallback:</strong> The logs show the system constantly asking the user for manual input (<code class="language-plaintext highlighter-rouge">Please manually input the folder label</code>), essentially defeating the purpose of an <em>automatic</em> organizer.</li>
</ul>

<hr />

<p>## üõë Strategic Shift: Supervised Fine-Tuning (SFT)</p>

<p><strong>Problem:</strong> We need the intelligence of an LLM to generate labels for unknown files, but we cannot afford the latency or rate limits of an API.
 <strong>Solution:</strong> <strong>Supervised Fine-Tuning (SFT)</strong> a local Small Language Model (SLM).</p>

<p>### Why SFT?</p>
<ol>
  <li><strong>Zero Latency:</strong> A local model (e.g., Llama-3-8B-Quantized or TinyLlama) running on the GPU/CPU has no network overhead.</li>
  <li><strong>No Rate Limits:</strong> We can classify 10,000 files in a row without asking permission or waiting for quotas.</li>
  <li><strong>Privacy:</strong> File contents never leave the user‚Äôs machine.</li>
</ol>

<p>### The Plan</p>
<ol>
  <li><strong>Data Collection:</strong> We have collected high-quality ‚ÄúEvent‚Äù data in <code class="language-plaintext highlighter-rouge">rl_events.jsonl</code> (Input Text -&gt; Predicted Label).</li>
  <li><strong>Dataset Creation:</strong> Format these events into an SFT dataset (Instruction Tuning format).
    <ul>
      <li><em>Input:</em> ‚ÄúClassify this text: {content_summary}‚Äù</li>
      <li><em>Output:</em> ‚Äú{label}‚Äù</li>
    </ul>
  </li>
  <li><strong>Fine-Tuning:</strong> Train a small, efficient model to replicate the decision-making of the larger Gemini model.</li>
  <li><strong>Deployment:</strong> Replace the <code class="language-plaintext highlighter-rouge">generate_label.py</code> API calls with a local inference function.</li>
</ol>

<hr />

<p>## üìú Original Architecture (Reference)</p>

<p>### Strategy: Epsilon-Greedy Bandit</p>
<ul>
  <li><strong>Action:</strong> Choose a Policy (A, B, or C).</li>
  <li><strong>Reward:</strong> 1 (Success/Correct Sort) or 0 (Failure/Manual Fix).</li>
  <li><strong>Goal:</strong> Maximize cumulative reward over time.</li>
</ul>

<p>### Policies
 | Policy | Threshold | Allow GenAI? | Description |
 |:‚Äî:|:‚Äî:|:‚Äî:|‚Äî|
 | <strong>A</strong> | 0.45 | <strong>Yes</strong> | Conservative. High overlap required. |
 | <strong>B</strong> | 0.40 | <strong>Yes</strong> | Balanced. |
 | <strong>C</strong> | 0.35 | <strong>No</strong>  | <strong>Efficient.</strong> Aggressive matching. Pure Vector Search. |</p>

<p><em>(Note: While logical, this architecture is currently paused in favor of the SFT migration.)</em></p>

</article>
</html>
