<!DOCTYPE html>
<html lang="">

<title>Reinforcement Learning Architecture | FileSense Documentation</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="color-scheme" content="light dark">
<meta name="author" content="Ayush Bhalerao">
<meta name="generator" content="Jekyll v3.10.0">
<link rel="canonical" href="https://ahhyoushh.github.io/FileSense/wiki/rl/"><link rel="stylesheet" href="/FileSense/assets/css/index.css"><link rel="stylesheet" href="/FileSense/assets/PT-Sans/index.css"><link rel="alternate" href="/FileSense/feed.xml" type="application/atom+xml" title="FileSense Documentation"><link rel="stylesheet" href="/FileSense/assets/css/sidebar.css" media="screen and (min-width: 58em)">
<aside style="display: none">
  <nav>
  
  <a href="/FileSense/wiki/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Home</span>
    </a><a href="/FileSense/wiki/getting-started/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Getting Started</span>
    </a><a href="/FileSense/wiki/faq/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>FAQ</span>
    </a><a href="/FileSense/wiki/pipeline/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Architecture</span>
    </a><a href="/FileSense/wiki/rl/" class="selected">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Reinforcement Learning</span>
    </a><a href="/FileSense/wiki/metrics/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Metrics</span>
    </a><a href="/FileSense/wiki/NL_VS_OG/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>NL vs Keywords</span>
    </a><a href="/FileSense/wiki/lessons-learned/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Lessons Learned</span>
    </a><a href="https://github.com/ahhyoushh/FileSense" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#github"></use></svg>
      <span>GitHub</span>
    </a></nav>
  <footer>Intelligent semantic file organizer powered by SBERT + Gemini</footer>
</aside>


<header class="hover smooth">
  
    <h1 ><a href="/FileSense/">FileSense Documentation</a></h1>
  
  <nav><a href="/FileSense/wiki/">Home</a><a href="/FileSense/wiki/getting-started/">Getting Started</a><a href="/FileSense/wiki/faq/">FAQ</a><a href="/FileSense/wiki/pipeline/">Architecture</a><a href="/FileSense/wiki/rl/">Reinforcement Learning</a><a href="/FileSense/wiki/metrics/">Metrics</a><a href="/FileSense/wiki/NL_VS_OG/">NL vs Keywords</a><a href="/FileSense/wiki/lessons-learned/">Lessons Learned</a><a href="https://github.com/ahhyoushh/FileSense"><svg aria-label="GitHub" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#github"></use></svg></a></nav>
  
</header>

<article>
  <header>
  <h1><a href="/FileSense/wiki/rl/">Reinforcement Learning Architecture</a></h1>
</header>

  <h1 id="reinforcement-learning-integration">Reinforcement Learning Integration</h1>

<h2 id="-the-epsilon-greedy-bandit-agent">üß† The Epsilon-Greedy Bandit Agent</h2>
<p>FileSense has permanently evolved from a static script to an <strong>adaptive intelligent system</strong>. Integrated a <strong>Reinforcement Learning (RL)</strong> agent based on the <strong>Epsilon-Greedy Bandit</strong> algorithm.</p>

<h3 id="core-logic">Core Logic</h3>
<p>The agent‚Äôs goal is to maximize <strong>Accuracy</strong> while minimizing <strong>Latency</strong> and <strong>API Costs</strong>. It achieves this by dynamically choosing between three operating policies for every file it encounters:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Policy</th>
      <th style="text-align: center">Threshold</th>
      <th style="text-align: center">Allow GenAI?</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>A</strong></td>
      <td style="text-align: center">0.45</td>
      <td style="text-align: center"><strong>Yes</strong></td>
      <td><strong>Conservative.</strong> Requires high similarity to exist. Uses API frequently for new concepts.</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>B</strong></td>
      <td style="text-align: center">0.40</td>
      <td style="text-align: center"><strong>Yes</strong></td>
      <td><strong>Balanced.</strong> A middle ground between strictness and autonomy.</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>C</strong></td>
      <td style="text-align: center">0.35</td>
      <td style="text-align: center"><strong>No</strong></td>
      <td><strong>Efficient.</strong> Aggressive vector matching. Strictly forbids API calls to ensure speed.</td>
    </tr>
  </tbody>
</table>

<h3 id="the-learning-loop">The Learning Loop</h3>
<ol>
  <li><strong>State:</strong> The system observes a new file.</li>
  <li><strong>Action:</strong> The Agent selects a policy (A, B, or C).
    <ul>
      <li><em>Exploration:</em> Tries random policies to discover new efficiencies (Epsilon = 10%).</li>
      <li><em>Exploitation:</em> Chooses the best-known policy for reliability (90%).</li>
    </ul>
  </li>
  <li><strong>Reward:</strong>
    <ul>
      <li><strong>+1 (Success):</strong> File was sorted correctly without manual intervention.</li>
      <li><strong>0 (Failure):</strong> File required manual sorting or API failed.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="-the-rate-limit-bottleneck-why-paused">üöß The Rate Limit Bottleneck (Why paused)</h2>

<p>While the RL architecture is sound and fully implemented, I hit a <strong>hard external constraint</strong> during real-world testing.</p>

<h3 id="the-conflict-rl-speed-vs-api-limits">The Conflict: RL Speed vs. API Limits</h3>
<p>Reinforcement Learning requires rapid feedback loops (trial and error) to converge on an optimal policy. However, the <strong>free tier of Google Gemini API</strong> imposes severe rate limits (~15 RPM or fewer depending on load).</p>

<p><strong>Evidence from Logs:</strong></p>
<blockquote>
  <p><code class="language-plaintext highlighter-rouge">Error: 429 RESOURCE_EXHAUSTED ... limit: 20 requests/day ... Please retry in 43.82s</code></p>
</blockquote>

<p>When the RL agent attempted to ‚ÄúExplore‚Äù (use GenAI) or when valid files needed labeling, the API would block the request for 40-60 seconds. This destroyed the reward signal:</p>
<ul>
  <li>The Agent successfully prioritized <strong>Policy C</strong> (No API) because it was the only one that didn‚Äôt crash.</li>
  <li>However, I <em>need</em> GenAI for that 10% of unknown files. Can‚Äôt simply turn it off.</li>
</ul>

<h3 id="conclusion-the-architecture-works-the-infrastructure-failed">Conclusion: The Architecture works, the Infrastructure failed.</h3>
<p>The RL implementation works perfectly: it correctly identified that API calls were ‚Äúexpensive‚Äù (in time). The problem is that the ‚Äúcost‚Äù (60s wait) was too high for a production application.</p>

<hr />

<h2 id="-strategic-shift-local-sft">üõë Strategic Shift: Local SFT</h2>

<p>To unblock the RL capabilities, I‚Äôm removing the bottleneck.</p>

<p><strong>The Solution: Supervised Fine-Tuning (SFT)</strong>
Moving the intelligence from the Cloud (Gemini) to the Edge (Local LLM).</p>
<ol>
  <li><strong>Fine-Tune a Local Model (Llama-3-8B):</strong> Using the data collected by the RL agent.</li>
  <li><strong>Remove API Dependency:</strong> Local models have no rate limits.</li>
  <li><strong>Re-Activate RL:</strong> Once the ‚Äúcost‚Äù of generation drops from 60s to 2s, the RL Agent will be fully re-enabled to manage local compute resources instead of API quotas.</li>
</ol>

<p><strong>The RL module remains a permanent, core part of FileSense‚Äôs codebase, awaiting the local model integration.</strong></p>

</article>
</html>
