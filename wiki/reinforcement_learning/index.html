<!DOCTYPE html>
<html lang="">

<title>Reinforcement Learning Architecture | FileSense Documentation</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="color-scheme" content="light dark">
<meta name="author" content="Ayush Bhalerao">
<meta name="generator" content="Jekyll v3.10.0">
<link rel="canonical" href="https://ahhyoushh.github.io/FileSense/wiki/reinforcement_learning/"><link rel="stylesheet" href="/FileSense/assets/css/index.css"><link rel="stylesheet" href="/FileSense/assets/PT-Sans/index.css"><link rel="alternate" href="/FileSense/feed.xml" type="application/atom+xml" title="FileSense Documentation"><link rel="stylesheet" href="/FileSense/assets/css/sidebar.css" media="screen and (min-width: 58em)">
<aside style="display: none">
  <nav>
  
  <a href="/FileSense/wiki/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Home</span>
    </a><a href="/FileSense/wiki/getting-started/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Getting Started</span>
    </a><a href="/FileSense/wiki/faq/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>FAQ</span>
    </a><a href="/FileSense/wiki/pipeline/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Architecture</span>
    </a><a href="/FileSense/wiki/metrics/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Metrics</span>
    </a><a href="/FileSense/wiki/NL_VS_OG/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>NL vs Keywords</span>
    </a><a href="/FileSense/wiki/lessons-learned/" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#"></use></svg>
      <span>Lessons Learned</span>
    </a><a href="https://github.com/ahhyoushh/FileSense" class="">
      <svg aria-hidden="true" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#github"></use></svg>
      <span>GitHub</span>
    </a></nav>
  <footer>Intelligent semantic file organizer powered by SBERT + Gemini</footer>
</aside>


<header class="hover smooth">
  
    <h1 ><a href="/FileSense/">FileSense Documentation</a></h1>
  
  <nav><a href="/FileSense/wiki/">Home</a><a href="/FileSense/wiki/getting-started/">Getting Started</a><a href="/FileSense/wiki/faq/">FAQ</a><a href="/FileSense/wiki/pipeline/">Architecture</a><a href="/FileSense/wiki/metrics/">Metrics</a><a href="/FileSense/wiki/NL_VS_OG/">NL vs Keywords</a><a href="/FileSense/wiki/lessons-learned/">Lessons Learned</a><a href="https://github.com/ahhyoushh/FileSense"><svg aria-label="GitHub" width="1em" height="1em"><use xlink:href="/FileSense/assets/fontawesome/icons.svg#github"></use></svg></a></nav>
  
</header>

<article>
  <header>
  <h1><a href="/FileSense/wiki/reinforcement_learning/">Reinforcement Learning Architecture</a></h1>
</header>

  <h1 id="reinforcement-learning-architecture">Reinforcement Learning Architecture</h1>

<h2 id="overview">Overview</h2>
<p>To optimize the trade-off between classification accuracy and system efficiency (latency/cost), FileSense utilizes a <strong>Reinforcement Learning (RL)</strong> agent. The agent dynamically selects the best configuration policy for classifying documents based on historical rewards.</p>

<h2 id="1-strategy-epsilon-greedy-bandit">1. Strategy: Epsilon-Greedy Bandit</h2>
<p>The system uses an <strong>ε-greedy (epsilon-greedy)</strong> bandit algorithm, a practical and robust strategy for online decision making.</p>

<h3 id="how-it-works">How it Works</h3>
<p>At each decision point (serving a file):</p>
<ul>
  <li><strong>Exploration (ε = 0.10):</strong> With 10% probability, the agent picks a random policy to discover potential improvements.</li>
  <li><strong>Exploitation (1 - ε = 0.90):</strong> With 90% probability, the agent picks the best-performing policy based on historical average rewards.</li>
</ul>

<p>This ensures the system mostly uses the optimal configuration while never stopping the search for a better one.</p>

<hr />

<h2 id="2-policies">2. Policies</h2>
<p>The policies represent different configurations of the classification engine. They vary in aggressiveness (thresholds) and cost (whether generation is allowed).</p>

<table>
  <thead>
    <tr>
      <th>Policy ID</th>
      <th>Threshold</th>
      <th>Low Conf.</th>
      <th>Generative?</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Policy A</strong></td>
      <td>0.45</td>
      <td>0.40</td>
      <td><strong>Yes</strong></td>
      <td>Conservative. High overlap required. Falls back to LLM generation if needed.</td>
    </tr>
    <tr>
      <td><strong>Policy B</strong></td>
      <td>0.40</td>
      <td>0.30</td>
      <td><strong>Yes</strong></td>
      <td>Balanced. Moderate overlap accepted. LLM generation allowed.</td>
    </tr>
    <tr>
      <td><strong>Policy C</strong></td>
      <td>0.35</td>
      <td>0.25</td>
      <td><strong>No</strong></td>
      <td><strong>Efficient.</strong> Aggressive matching. <strong>LLM Generation Disabled.</strong> Relies purely on embeddings.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="3-efficiency-analysis-generation-free-classification">3. Efficiency Analysis: Generation-Free Classification</h2>
<p>A key goal of the RL agent is to identify if and when expensive Generative AI calls can be avoided.</p>

<p><strong>Policy C</strong> is designed specifically for this purpose. It disables <code class="language-plaintext highlighter-rouge">ALLOW_GENERATION</code>, forcing the system to rely solely on vector similarity scores and lower thresholds.</p>

<h3 id="success-without-generation">Success without Generation</h3>
<p>Initial evaluation logs demonstrate that <strong>Policy C</strong> is highly effective for standard academic datasets (e.g., NCERT, STEM).</p>
<ul>
  <li><strong>Efficiency:</strong> By skipping the generation step, Policy C reduces processing time per file significantly (removing the 2-5s latency of LLM calls).</li>
  <li><strong>Accuracy:</strong> Despite the lack of generation, the adjusted thresholds (0.35) allow for successful classification purely based on semantic alignment between document text and folder labels.</li>
</ul>

<p><strong>Conclusion:</strong> The RL agent successfully identifies that for well-defined domains, <strong>retrieval-based classification (Policy C)</strong> is sufficient and far more efficient than generative approaches.</p>

<hr />

<h2 id="4-work-flow">4. Work Flow</h2>

<p>The RL decision process follows this loop:</p>

<pre><code class="language-mermaid">graph TD
    A[Start: Serve File] --&gt; B{Choose Policy}
    B -- Exploit (90%) --&gt; C[Best Known Policy]
    B -- Explore (10%) --&gt; D[Random Policy]
    C --&gt; E[Run FileSense Classification]
    D --&gt; E
    E --&gt; F[Log 'Served' Event]
    F --&gt; G[Move File]
    G --&gt; H[Audit Phase]
    H --&gt; I{Correct Label?}
    I -- Yes --&gt; J[Reward = 1.0]
    I -- No --&gt; K[Reward = 0.0]
    J --&gt; L[Update Policy Stats]
    K --&gt; L
    L --&gt; M[Next Decision Uses Updated Stats]
</code></pre>

<hr />

<h2 id="5-performance-stats">5. Performance Stats</h2>
<p><em>Snapshot from <code class="language-plaintext highlighter-rouge">rl_policy_stats.json</code></em></p>

<table>
  <thead>
    <tr>
      <th>Policy</th>
      <th>Count</th>
      <th>Total Reward</th>
      <th>Avg Reward</th>
      <th>Evaluation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Policy A</strong></td>
      <td>17</td>
      <td>14.4</td>
      <td><strong>0.85</strong></td>
      <td>Stable, reliable, high cost.</td>
    </tr>
    <tr>
      <td><strong>Policy B</strong></td>
      <td>1</td>
      <td>0.4</td>
      <td><strong>0.40</strong></td>
      <td>Underperforming in current tests.</td>
    </tr>
    <tr>
      <td><strong>Policy C</strong></td>
      <td>High*</td>
      <td>High*</td>
      <td><strong>1.00</strong></td>
      <td><strong>Optimal Efficiency.</strong> High usage in recent events.</td>
    </tr>
  </tbody>
</table>

<p><em>*Note: Recent logs show a surge in Policy C usage, indicating the agent is converging on this efficient strategy.</em></p>

</article>
</html>
