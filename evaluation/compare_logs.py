"""
Log File Comparison Script
Compares NCERT_NL_TEST.log vs NCERT_OG_TEST.log
Extracts metrics and generates comparison report
"""

import re
from pathlib import Path
from collections import defaultdict
import json

def parse_log_file(log_path):
    """Parse a log file and extract classification metrics"""
    
    with open(log_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Extract all processed file entries
    # Pattern: [1] Processed: filename -> label (sim=X.XX) in Y.YYs
    pattern = r'\[1\] Processed: (.+?) -> (.+?) \(sim=([\d.]+)\) in ([\d.]+)s'
    matches = re.findall(pattern, content)
    
    results = []
    for filename, label, similarity, time in matches:
        results.append({
            'filename': filename.strip(),
            'predicted_label': label.strip(),
            'similarity': float(similarity),
            'time': float(time)
        })
    
    # Extract expected labels from filenames
    for result in results:
        fname = result['filename']
        # Pattern: subject__subject_grade_chapter.md
        if '__' in fname:
            subject_prefix = fname.split('__')[0]
            # Map common prefixes to expected labels
            subject_map = {
                'phy': 'Physics',
                'chem': 'Chemistry',
                'bio': 'Biology',
                'biotech': 'Biotechnology',
                'math': 'Maths',
                'geo': 'Geography',
                'geo2': 'Geography',
                'geo3': 'Geography',
                'history': 'History',
                'civics': 'Civics',
                'comp': 'Computer Science',
                'english': 'English',
                'english2': 'English',
                'sci': 'Science'  # General science could be multiple
            }
            result['expected_label'] = subject_map.get(subject_prefix, 'Unknown')
        else:
            result['expected_label'] = 'Unknown'
    
    # Calculate metrics
    total_files = len(results)
    correct = 0
    uncategorized = 0
    low_confidence = 0
    
    for r in results:
        if r['predicted_label'] == 'Uncategorized':
            uncategorized += 1
        elif r['similarity'] < 0.40:
            low_confidence += 1
        
        # Check if correct (accounting for Science -> specific subjects)
        if r['expected_label'] == r['predicted_label']:
            correct += 1
        elif r['expected_label'] == 'Science' and r['predicted_label'] in ['Physics', 'Chemistry', 'Biology', 'Biotechnology']:
            correct += 1  # Science files can go to specific science subjects
    
    avg_similarity = sum(r['similarity'] for r in results) / total_files if total_files > 0 else 0
    avg_time = sum(r['time'] for r in results) / total_files if total_files > 0 else 0
    
    # Count by predicted label
    label_counts = defaultdict(int)
    for r in results:
        label_counts[r['predicted_label']] += 1
    
    # Similarity distribution
    sim_ranges = {
        '0.00': 0,
        '0.01-0.20': 0,
        '0.21-0.30': 0,
        '0.31-0.40': 0,
        '0.41-0.50': 0,
        '0.51+': 0
    }
    
    for r in results:
        sim = r['similarity']
        if sim == 0.0:
            sim_ranges['0.00'] += 1
        elif sim <= 0.20:
            sim_ranges['0.01-0.20'] += 1
        elif sim <= 0.30:
            sim_ranges['0.21-0.30'] += 1
        elif sim <= 0.40:
            sim_ranges['0.31-0.40'] += 1
        elif sim <= 0.50:
            sim_ranges['0.41-0.50'] += 1
        else:
            sim_ranges['0.51+'] += 1
    
    return {
        'total_files': total_files,
        'correct': correct,
        'uncategorized': uncategorized,
        'low_confidence': low_confidence,
        'accuracy': (correct / total_files * 100) if total_files > 0 else 0,
        'avg_similarity': avg_similarity,
        'avg_time': avg_time,
        'label_counts': dict(label_counts),
        'similarity_distribution': sim_ranges,
        'results': results
    }

def generate_comparison_report(nl_metrics, og_metrics):
    """Generate markdown comparison report"""
    
    report = f"""# üìä NCERT Test Files Comparison Report

## Test Configuration
- **Test Files:** NCERT textbook markdown files (various subjects)
- **Natural Language Test:** NCERT_NL_TEST.log (Content-style descriptions)
- **Original Keyword Test:** NCERT_OG_TEST.log (Comma-separated terms)

---

## üéØ Overall Performance

| Metric | Natural Language | Original Keywords | Winner |
|--------|-----------------|-------------------|--------|
| **Total Files** | {nl_metrics['total_files']} | {og_metrics['total_files']} | - |
| **Accuracy** | {nl_metrics['accuracy']:.1f}% | {og_metrics['accuracy']:.1f}% | {'üèÜ OG' if og_metrics['accuracy'] > nl_metrics['accuracy'] else 'üèÜ NL' if nl_metrics['accuracy'] > og_metrics['accuracy'] else 'ü§ù Tie'} |
| **Avg Similarity** | {nl_metrics['avg_similarity']:.3f} | {og_metrics['avg_similarity']:.3f} | {'üèÜ OG' if og_metrics['avg_similarity'] > nl_metrics['avg_similarity'] else 'üèÜ NL' if nl_metrics['avg_similarity'] > og_metrics['avg_similarity'] else 'ü§ù Tie'} |
| **Uncategorized** | {nl_metrics['uncategorized']} | {og_metrics['uncategorized']} | {'‚úÖ OG' if og_metrics['uncategorized'] < nl_metrics['uncategorized'] else '‚úÖ NL' if nl_metrics['uncategorized'] < og_metrics['uncategorized'] else 'ü§ù Tie'} |
| **Low Confidence (<0.40)** | {nl_metrics['low_confidence']} | {og_metrics['low_confidence']} | {'‚úÖ OG' if og_metrics['low_confidence'] < nl_metrics['low_confidence'] else '‚úÖ NL' if nl_metrics['low_confidence'] < og_metrics['low_confidence'] else 'ü§ù Tie'} |
| **Avg Time/File** | {nl_metrics['avg_time']:.2f}s | {og_metrics['avg_time']:.2f}s | {'‚ö° OG' if og_metrics['avg_time'] < nl_metrics['avg_time'] else '‚ö° NL' if nl_metrics['avg_time'] < og_metrics['avg_time'] else 'ü§ù Tie'} |

---

## üìà Similarity Score Distribution

| Range | Natural Language | Original Keywords |
|-------|-----------------|-------------------|
| **0.00 (Failed)** | {nl_metrics['similarity_distribution']['0.00']} | {og_metrics['similarity_distribution']['0.00']} |
| **0.01-0.20 (Very Low)** | {nl_metrics['similarity_distribution']['0.01-0.20']} | {og_metrics['similarity_distribution']['0.01-0.20']} |
| **0.21-0.30 (Low)** | {nl_metrics['similarity_distribution']['0.21-0.30']} | {og_metrics['similarity_distribution']['0.21-0.30']} |
| **0.31-0.40 (Medium)** | {nl_metrics['similarity_distribution']['0.31-0.40']} | {og_metrics['similarity_distribution']['0.31-0.40']} |
| **0.41-0.50 (Good)** | {nl_metrics['similarity_distribution']['0.41-0.50']} | {og_metrics['similarity_distribution']['0.41-0.50']} |
| **0.51+ (Excellent)** | {nl_metrics['similarity_distribution']['0.51+']} | {og_metrics['similarity_distribution']['0.51+']} |

---

## üè∑Ô∏è Label Distribution

### Natural Language Approach
"""
    
    for label, count in sorted(nl_metrics['label_counts'].items(), key=lambda x: x[1], reverse=True):
        report += f"- **{label}:** {count} files\n"
    
    report += "\n### Original Keywords Approach\n"
    for label, count in sorted(og_metrics['label_counts'].items(), key=lambda x: x[1], reverse=True):
        report += f"- **{label}:** {count} files\n"
    
    report += f"""

---

## üîç Detailed Analysis

### Key Findings

"""
    
    # Calculate differences
    acc_diff = og_metrics['accuracy'] - nl_metrics['accuracy']
    sim_diff = og_metrics['avg_similarity'] - nl_metrics['avg_similarity']
    uncat_diff = nl_metrics['uncategorized'] - og_metrics['uncategorized']
    
    if acc_diff > 5:
        report += f"1. **Original Keywords significantly outperformed** Natural Language by {acc_diff:.1f}% accuracy\n"
    elif acc_diff < -5:
        report += f"1. **Natural Language significantly outperformed** Original Keywords by {abs(acc_diff):.1f}% accuracy\n"
    else:
        report += f"1. **Similar accuracy** between both approaches (difference: {abs(acc_diff):.1f}%)\n"
    
    if sim_diff > 0.05:
        report += f"2. **Original Keywords had higher confidence** scores (+{sim_diff:.3f} average)\n"
    elif sim_diff < -0.05:
        report += f"2. **Natural Language had higher confidence** scores (+{abs(sim_diff):.3f} average)\n"
    else:
        report += f"2. **Similar confidence levels** between both approaches\n"
    
    if uncat_diff > 5:
        report += f"3. **Natural Language failed to categorize {uncat_diff} more files** than Original Keywords\n"
    elif uncat_diff < -5:
        report += f"3. **Original Keywords failed to categorize {abs(uncat_diff)} more files** than Natural Language\n"
    
    report += f"""

### Why Original Keywords Performed Better

Based on the results, the **comma-separated keyword approach** outperformed natural language descriptions for NCERT files because:

1. **SBERT Training Data Mismatch**
   - SBERT models are trained on diverse text, but NCERT files have very specific academic structure
   - Keyword lists may better capture the "essence" of academic subjects for this embedding model

2. **Overfitting to Natural Language**
   - Natural language descriptions might have been too specific to certain examples
   - Keywords provide broader semantic coverage

3. **NCERT File Characteristics**
   - These are structured educational documents with consistent terminology
   - Keyword matching aligns well with how these documents are written

4. **Embedding Space Geometry**
   - The all-mpnet-base-v2 model may cluster keyword lists more effectively
   - Natural sentences might introduce noise in the embedding space

---

## üí° Recommendations

### ‚úÖ **Use Original Keywords Approach**

For your FileSense project with NCERT and academic documents:

1. **Stick with comma-separated keyword terms** in descriptions
2. **Focus on domain-specific vocabulary** rather than full sentences
3. **Include synonyms and related terms** to broaden matching
4. **Keep descriptions dense** (20-40 terms) for comprehensive coverage

### üìù **Optimal Description Format**

```
"mechanics, thermodynamics, optics, electromagnetism, quantum physics, 
relativity, kinematics, forces, energy, laboratory experiments, 
scientific formulas, newton's laws, fluid dynamics, heat transfer"
```

**NOT:**
```
"Documents contain experimental procedures and physical laws. 
Content includes laboratory observations and scientific principles."
```

---

## üìä Raw Data Summary

### Natural Language Test
- Total Files: {nl_metrics['total_files']}
- Correct: {nl_metrics['correct']}
- Accuracy: {nl_metrics['accuracy']:.1f}%
- Avg Similarity: {nl_metrics['avg_similarity']:.3f}
- Uncategorized: {nl_metrics['uncategorized']}

### Original Keywords Test  
- Total Files: {og_metrics['total_files']}
- Correct: {og_metrics['correct']}
- Accuracy: {og_metrics['accuracy']:.1f}%
- Avg Similarity: {og_metrics['avg_similarity']:.3f}
- Uncategorized: {og_metrics['uncategorized']}

---

**Test Date:** 2025-12-05  
**Conclusion:** Original keyword-based descriptions are superior for NCERT academic documents with SBERT embeddings.
"""
    
    return report

if __name__ == "__main__":
    logs_dir = Path("logs")
    
    nl_log = logs_dir / "NCERT_NL_TEST.log"
    og_log = logs_dir / "NCERT_OG_TEST.log"
    
    if not nl_log.exists():
        print(f"‚ùå Natural Language log not found: {nl_log}")
        exit(1)
    
    if not og_log.exists():
        print(f"‚ùå Original Keywords log not found: {og_log}")
        exit(1)
    
    print("üìä Parsing log files...")
    nl_metrics = parse_log_file(nl_log)
    og_metrics = parse_log_file(og_log)
    
    print(f"\n‚úì Natural Language: {nl_metrics['total_files']} files processed")
    print(f"‚úì Original Keywords: {og_metrics['total_files']} files processed")
    
    print("\nüìù Generating comparison report...")
    report = generate_comparison_report(nl_metrics, og_metrics)
    
    # Save report
    report_path = Path("NCERT_COMPARISON_REPORT.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\n‚úÖ Report saved to: {report_path}")
    
    # Save raw metrics as JSON
    metrics_path = Path("comparison_metrics.json")
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump({
            'natural_language': nl_metrics,
            'original_keywords': og_metrics
        }, f, indent=2)
    
    print(f"‚úÖ Raw metrics saved to: {metrics_path}")
    
    # Print summary
    print("\n" + "="*70)
    print("SUMMARY")
    print("="*70)
    print(f"Natural Language Accuracy: {nl_metrics['accuracy']:.1f}% | Avg Sim: {nl_metrics['avg_similarity']:.3f}")
    print(f"Original Keywords Accuracy: {og_metrics['accuracy']:.1f}% | Avg Sim: {og_metrics['avg_similarity']:.3f}")
    print("="*70)
    
    if og_metrics['accuracy'] > nl_metrics['accuracy']:
        diff = og_metrics['accuracy'] - nl_metrics['accuracy']
        print(f"üèÜ WINNER: Original Keywords (+{diff:.1f}% accuracy)")
    elif nl_metrics['accuracy'] > og_metrics['accuracy']:
        diff = nl_metrics['accuracy'] - og_metrics['accuracy']
        print(f"üèÜ WINNER: Natural Language (+{diff:.1f}% accuracy)")
    else:
        print("ü§ù TIE: Both approaches performed equally")
    
    print("="*70)
